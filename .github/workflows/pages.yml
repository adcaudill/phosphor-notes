name: Deploy Jekyll site to Pages

on:
  push:
    branches: ['main']
    paths:
      - 'docs/**' # Trigger only if files in the 'docs' directory change
  pull_request:
    paths:
      - 'docs/**' # Run build for PRs targeting main when docs change
  # Allows you to run this workflow manually from the Actions tab
  workflow_dispatch:

# Sets permissions of the GITHUB_TOKEN to allow deployment to GitHub Pages
permissions:
  contents: read
  pages: write
  id-token: write

# Allow one concurrent deployment
concurrency:
  group: 'pages'
  cancel-in-progress: true

jobs:
  # Build job
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v6
        with:
          fetch-depth: 0 # Fetch all history for accurate last-modified dates
      - name: Setup Ruby
        uses: ruby/setup-ruby@v1
        with:
          ruby-version: '3.3' # Not needed with a .ruby-version file
          bundler-cache: true # runs 'bundle install' and caches installed gems automatically
          cache-version: 1 # Increment this number if you need to re-download cached gems
          working-directory: docs # Run bundler in the docs directory where the Gemfile lives
      - name: Setup Pages
        id: pages
        uses: actions/configure-pages@v5
      - name: Build with Jekyll
        # Build from within 'docs' (Gemfile/config there) and output to repo-root './_site'
        working-directory: docs
        run: bundle exec jekyll build --destination ../_site --baseurl "${{ steps.pages.outputs.base_path }}"
        env:
          JEKYLL_ENV: production
      - name: Fix sitemap for search engines
        run: |
          if [ -f ./_site/sitemap.xml ]; then
            sed -i 's/ xmlns:xsi="[^"]*" xsi:schemaLocation="[^"]*"//g' ./_site/sitemap.xml
            echo "Sitemap namespace attributes removed for better search engine compatibility"
          fi
      - name: Update robots.txt
        # The sitemap plugin generates a robots.txt with a sitemap URL, but nothing else.
        # This step ensures that the robots.txt allows crawling of the site and includes the sitemap URL.
        run: |
          touch ./_site/robots.txt
          echo "User-agent: *" > ./_site/robots.txt
          echo "Allow: /" >> ./_site/robots.txt
          echo "Sitemap: ${{ steps.pages.outputs.page_url }}/sitemap.xml" >> ./_site/robots.txt
          echo "robots.txt updated with sitemap URL"
      - name: Upload artifact
        # Automatically uploads an artifact from the './_site' directory by default
        uses: actions/upload-pages-artifact@v4

  # Deployment job
  deploy:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
